{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 30\n",
    "learning_rate = 0.005\n",
    "hidden_units = 128\n",
    "beta1 = 0.90\n",
    "beta2 = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "<>:5: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "/var/folders/xy/2nl06h6134z8d63822qpjs840000gn/T/ipykernel_25573/3253852577.py:5: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  return 1/1(1+np.exp(-X))\n"
     ]
    }
   ],
   "source": [
    "# Activation Functions\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1/1(1+np.exp(-X))\n",
    "\n",
    "def tanh_activation(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "# softmax activation\n",
    "def softmax(X):\n",
    "    exp_X = np.exp(X)\n",
    "    exp_X_sum = np.sum(exp_X, axis=1).reshape(-1, 1)\n",
    "    exp_X = exp_X / exp_X_sum\n",
    "    return exp_X\n",
    "\n",
    "# derivative of tanh\n",
    "def tanh_derivative(X):\n",
    "    return 1 - (X**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 lstm，包含cell state, hidden state\n",
    "def init_lstm_state(batch_size, hidden_units, device):\n",
    "    return (torch.zeros((batch_size, hidden_units), ctx=device), \n",
    "            torch.zeros((batch_size, hidden_units), ctx=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "def initialize_parameters(vocab_size, hidden_units, device):\n",
    "    std = 0.01\n",
    "    input_units = output_units = vocab_size\n",
    "\n",
    "    # 正态分布\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * std\n",
    "\n",
    "    # LSTM cell weights\n",
    "    forget_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    input_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    output_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    c_tilda_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "\n",
    "    # 偏置项\n",
    "    forget_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    input_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    output_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    c_tilda_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "\n",
    "    # 输出层参数\n",
    "    hidden_output_weights = normal((hidden_units, output_units))\n",
    "    output_bias = torch.zeros((1, output_units), device=device)\n",
    "\n",
    "    # 将所有参数添加到字典\n",
    "    parameters = {\n",
    "        'fgw': forget_gate_weights,\n",
    "        'igw': input_gate_weights,\n",
    "        'ogw': output_gate_weights,\n",
    "        'cgw': c_tilda_gate_weights,\n",
    "        'fgb': forget_gate_bias,\n",
    "        'igb': input_gate_bias,\n",
    "        'ogb': output_gate_bias,\n",
    "        'cgb': c_tilda_gate_bias,\n",
    "        'how': hidden_output_weights,\n",
    "        'ob': output_bias\n",
    "    }\n",
    "\n",
    "    # 设置 requires_grad=True 以启用梯度计算\n",
    "    # 确保所有参数在反向传播中能够计算梯度\n",
    "    for param in parameters.values():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm cell\n",
    "def lstm_cell(batch_dataset, prev_hidden_state, prev_cell_state, parameters):\n",
    "    # get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    cgw = parameters['cgw']\n",
    "\n",
    "    fgb = parameters['fgb']\n",
    "    igb = parameters['igb']\n",
    "    ogb = parameters['ogb']\n",
    "    cgb = parameters['cgb']\n",
    "    \n",
    "    # 串联 data 和 prev_hidden_state\n",
    "    concat_dataset = np.concatenate((batch_dataset, prev_hidden_state), axis=1)\n",
    "\n",
    "    # forget gate activations\n",
    "    F = sigmoid(np.matmul(concat_dataset, fgw) + fgb)\n",
    "\n",
    "    # input gate activations\n",
    "    I = sigmoid(np.matmul(concat_dataset, igw) + igb)\n",
    "\n",
    "    # output gate activations\n",
    "    O = sigmoid(np.matmul(concat_dataset, ogw) + ogb)\n",
    "\n",
    "    # cell_tilda gate activations\n",
    "    C_tilda = np.tanh(np.matmul(concat_dataset, cgw) + cgb)\n",
    "\n",
    "    # 更新 cell state, hidden_state\n",
    "    cell_state = F * prev_cell_state + I * C_tilda\n",
    "    hidden_state = np.multiply(O, np.tanh(cell_state))\n",
    "\n",
    "    # store four gate weights to be used in back propagation\n",
    "    lstm_activations = {\n",
    "        'F': F,\n",
    "        'I': I,\n",
    "        'O': O,\n",
    "        'C_tilda': C_tilda\n",
    "    }\n",
    "    \n",
    "    return lstm_activations, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出层\n",
    "# 需要注意的是，只有隐状态才会传递到输出层，而记忆元不直接参与输出计算，记忆元完全属于内部信息\n",
    "def output_cell(hidden_state, parameters):\n",
    "    # get hidden to output parameters\n",
    "    how = parameters['how']\n",
    "    ob = parameters['ob']\n",
    "    # calculate the output\n",
    "    output = np.matmul(hidden_state, how)\n",
    "    # 如果输出为概率的话，可以使用softmax函数进行归一化\n",
    "    # output = softmax(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(inputs, initail_state, parameters):\n",
    "    # inputs的形状：(时间步数量， 批量大小， 词表大小) (num_steps, batch_size, vocab_size)\n",
    "    hidden_state, cell_state = initail_state\n",
    "    outputs = []\n",
    "\n",
    "    for X in inputs:\n",
    "        _, hidden_state, cell_state = lstm_cell(X, hidden_state, cell_state, parameters)\n",
    "    \n",
    "        outputs.append(output_cell(hidden_state, parameters))\n",
    "    return outputs, (hidden_state, cell_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个RNN 类来训练LSTM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModelScratch:\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.params = get_params(vocab_size, hidden_units, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "    \n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RNNModelScratch at 0x161427a00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModelScratch(vocab_size, hidden_units, device, initialize_parameters, init_lstm_state, lstm)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fgw': tensor([[-0.0008,  0.0131, -0.0069,  ...,  0.0121, -0.0083,  0.0055],\n",
       "         [-0.0076,  0.0060,  0.0019,  ..., -0.0173,  0.0004,  0.0165],\n",
       "         [ 0.0054,  0.0005, -0.0215,  ...,  0.0070, -0.0085,  0.0161],\n",
       "         ...,\n",
       "         [-0.0133,  0.0039,  0.0162,  ...,  0.0087, -0.0113, -0.0034],\n",
       "         [ 0.0032,  0.0027,  0.0012,  ...,  0.0015,  0.0125,  0.0067],\n",
       "         [ 0.0176, -0.0077, -0.0093,  ...,  0.0106,  0.0220, -0.0006]],\n",
       "        requires_grad=True),\n",
       " 'igw': tensor([[-0.0099, -0.0066,  0.0038,  ...,  0.0047, -0.0006,  0.0182],\n",
       "         [-0.0023, -0.0048,  0.0153,  ..., -0.0240,  0.0119,  0.0001],\n",
       "         [-0.0085, -0.0020,  0.0001,  ...,  0.0026,  0.0206, -0.0091],\n",
       "         ...,\n",
       "         [ 0.0029,  0.0095,  0.0106,  ...,  0.0017, -0.0030, -0.0011],\n",
       "         [ 0.0133, -0.0033,  0.0187,  ..., -0.0050,  0.0088,  0.0128],\n",
       "         [ 0.0067, -0.0166,  0.0203,  ..., -0.0119, -0.0224,  0.0087]],\n",
       "        requires_grad=True),\n",
       " 'ogw': tensor([[-0.0009,  0.0058,  0.0118,  ..., -0.0076, -0.0162, -0.0040],\n",
       "         [ 0.0089,  0.0043,  0.0071,  ...,  0.0059,  0.0126,  0.0026],\n",
       "         [ 0.0132, -0.0028,  0.0011,  ...,  0.0046,  0.0048, -0.0141],\n",
       "         ...,\n",
       "         [-0.0046, -0.0113, -0.0086,  ..., -0.0013, -0.0075,  0.0026],\n",
       "         [-0.0029, -0.0078,  0.0029,  ...,  0.0109, -0.0096,  0.0131],\n",
       "         [ 0.0170,  0.0055, -0.0027,  ...,  0.0059, -0.0195, -0.0120]],\n",
       "        requires_grad=True),\n",
       " 'cgw': tensor([[-0.0014,  0.0047, -0.0003,  ..., -0.0025,  0.0058, -0.0039],\n",
       "         [ 0.0004, -0.0208,  0.0062,  ..., -0.0042, -0.0107,  0.0116],\n",
       "         [-0.0179,  0.0062, -0.0032,  ...,  0.0048, -0.0049, -0.0130],\n",
       "         ...,\n",
       "         [ 0.0019,  0.0043,  0.0154,  ..., -0.0043,  0.0101, -0.0008],\n",
       "         [-0.0136,  0.0045, -0.0047,  ...,  0.0011,  0.0006,  0.0055],\n",
       "         [-0.0008, -0.0094, -0.0028,  ...,  0.0022,  0.0086,  0.0016]],\n",
       "        requires_grad=True),\n",
       " 'fgb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'igb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'ogb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'cgb': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True),\n",
       " 'how': tensor([[ 0.0199,  0.0092, -0.0094,  ...,  0.0103, -0.0136, -0.0087],\n",
       "         [-0.0019, -0.0163,  0.0026,  ...,  0.0260, -0.0082,  0.0076],\n",
       "         [ 0.0135, -0.0113, -0.0147,  ...,  0.0006,  0.0152, -0.0157],\n",
       "         ...,\n",
       "         [-0.0081, -0.0079,  0.0034,  ...,  0.0046, -0.0071, -0.0027],\n",
       "         [-0.0103,  0.0064, -0.0006,  ..., -0.0061, -0.0028,  0.0046],\n",
       "         [-0.0131,  0.0070,  0.0083,  ..., -0.0012,  0.0064,  0.0045]],\n",
       "        requires_grad=True),\n",
       " 'ob': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]], requires_grad=True)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'd2l.torch' has no attribute 'load_data_time_machine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md2l\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m torch \u001b[38;5;28;01mas\u001b[39;00m d2l\n\u001b[1;32m      3\u001b[0m batch_size, num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m35\u001b[39m\n\u001b[0;32m----> 4\u001b[0m train_iter, vocab \u001b[38;5;241m=\u001b[39m \u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_time_machine\u001b[49m(batch_size, num_steps)\n\u001b[1;32m      6\u001b[0m vocab_size, num_hiddens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab), \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m      7\u001b[0m num_epochs, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'd2l.torch' has no attribute 'load_data_time_machine'"
     ]
    }
   ],
   "source": [
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "vocab_size, num_hiddens = len(vocab), 256\n",
    "num_epochs, lr = 500, 1\n",
    "\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corresonding embeddings for the batch dataset\n",
    "def get_embeddings(batch_dataset, embeddings):\n",
    "    embedding_dataset = np.matmul(batch_dataset, embeddings)\n",
    "    return embedding_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![forward propagation](../assets/forward-propagation.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2557073817.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[64], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# forward propagation\n",
    "def forward_propagation(batches, parameters, embeddings):\n",
    "    # get batch size\n",
    "    batch_size = batches[0].shape[0]\n",
    "\n",
    "    # 存储缓存信息\n",
    "    lstm_cache = dict()\n",
    "    hidden_cache = dict()\n",
    "    cell_cache = dict()\n",
    "    output_cache = dict()\n",
    "    embedding_cache = dict()\n",
    "\n",
    "    # 初始化hidden_state(h0), cell_state(c0)  偏置项\n",
    "    h0 = np.zeros([batch_size, hidden_units], dtype=np.float32)\n",
    "    c0 = np.zeros([batch_size, hidden_units], dtype=np.float32)\n",
    "\n",
    "    # 存储初始的hidden_state(h0), cell_state(c0)\n",
    "    hidden_cache['h0'] = h0\n",
    "    cell_cache['c0'] = c0\n",
    "\n",
    "    for i in range(len(batches) - 1):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
