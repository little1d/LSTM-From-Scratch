{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 30\n",
    "learning_rate = 0.005\n",
    "hidden_units = 128\n",
    "beta1 = 0.90\n",
    "beta2 = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "<>:5: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "/var/folders/xy/2nl06h6134z8d63822qpjs840000gn/T/ipykernel_4816/3253852577.py:5: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  return 1/1(1+np.exp(-X))\n"
     ]
    }
   ],
   "source": [
    "# Activation Functions\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1/1(1+np.exp(-X))\n",
    "\n",
    "def tanh_activation(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "# softmax activation\n",
    "def softmax(X):\n",
    "    exp_X = np.exp(X)\n",
    "    exp_X_sum = np.sum(exp_X, axis=1).reshape(-1, 1)\n",
    "    exp_X = exp_X / exp_X_sum\n",
    "    return exp_X\n",
    "\n",
    "# derivative of tanh\n",
    "def tanh_derivative(X):\n",
    "    return 1 - (X**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 lstm，包含cell state, hidden state\n",
    "def init_lstm_state(batch_size, hidden_units, device):\n",
    "    return (torch.zeros((batch_size, hidden_units), ctx=device), \n",
    "            torch.zeros((batch_size, hidden_units), ctx=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "def initialize_parameters(vocab_size, hidden_units, device):\n",
    "    std = 0.01\n",
    "    input_units = output_units = vocab_size\n",
    "\n",
    "    # 正态分布\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * std\n",
    "\n",
    "    # LSTM cell weights\n",
    "    forget_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    input_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    output_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "    c_tilda_gate_weights = normal((input_units + hidden_units, hidden_units))\n",
    "\n",
    "    # 偏置项\n",
    "    forget_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    input_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    output_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "    c_tilda_gate_bias = torch.zeros((1, hidden_units), device=device)\n",
    "\n",
    "    # 输出层参数\n",
    "    hidden_output_weights = normal((hidden_units, output_units))\n",
    "    output_bias = torch.zeros((1, output_units), device=device)\n",
    "\n",
    "    # 将所有参数添加到字典\n",
    "    parameters = {\n",
    "        'fgw': forget_gate_weights,\n",
    "        'igw': input_gate_weights,\n",
    "        'ogw': output_gate_weights,\n",
    "        'cgw': c_tilda_gate_weights,\n",
    "        'fgb': forget_gate_bias,\n",
    "        'igb': input_gate_bias,\n",
    "        'ogb': output_gate_bias,\n",
    "        'cgb': c_tilda_gate_bias,\n",
    "        'how': hidden_output_weights,\n",
    "        'ob': output_bias\n",
    "    }\n",
    "\n",
    "    # 设置 requires_grad=True 以启用梯度计算\n",
    "    # 确保所有参数在反向传播中能够计算梯度\n",
    "    for param in parameters.values():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm cell\n",
    "def lstm_cell(batch_dataset, prev_hidden_state, prev_cell_state, parameters):\n",
    "    # get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    cgw = parameters['cgw']\n",
    "\n",
    "    fgb = parameters['fgb']\n",
    "    igb = parameters['igb']\n",
    "    ogb = parameters['ogb']\n",
    "    cgb = parameters['cgb']\n",
    "    \n",
    "    # 串联 data 和 prev_hidden_state\n",
    "    concat_dataset = np.concatenate((batch_dataset, prev_hidden_state), axis=1)\n",
    "\n",
    "    # forget gate activations\n",
    "    F = sigmoid(np.matmul(concat_dataset, fgw) + fgb)\n",
    "\n",
    "    # input gate activations\n",
    "    I = sigmoid(np.matmul(concat_dataset, igw) + igb)\n",
    "\n",
    "    # output gate activations\n",
    "    O = sigmoid(np.matmul(concat_dataset, ogw) + ogb)\n",
    "\n",
    "    # cell_tilda gate activations\n",
    "    C_tilda = np.tanh(np.matmul(concat_dataset, cgw) + cgb)\n",
    "\n",
    "    # 更新 cell state, hidden_state\n",
    "    cell_state = F * prev_cell_state + I * C_tilda\n",
    "    hidden_state = np.multiply(O, np.tanh(cell_state))\n",
    "\n",
    "    # store four gate weights to be used in back propagation\n",
    "    lstm_activations = {\n",
    "        'F': F,\n",
    "        'I': I,\n",
    "        'O': O,\n",
    "        'C_tilda': C_tilda\n",
    "    }\n",
    "    \n",
    "    return lstm_activations, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出层\n",
    "# 需要注意的是，只有隐状态才会传递到输出层，而记忆元不直接参与输出计算，记忆元完全属于内部信息\n",
    "def output_cell(hidden_state, parameters):\n",
    "    # get hidden to output parameters\n",
    "    how = parameters['how']\n",
    "    ob = parameters['ob']\n",
    "    # calculate the output\n",
    "    output = np.matmul(hidden_state, how)\n",
    "    # 如果输出为概率的话，可以使用softmax函数进行归一化\n",
    "    # output = softmax(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(inputs, initail_state, parameters):\n",
    "    # inputs的形状：(时间步数量， 批量大小， 词表大小) (num_steps, batch_size, vocab_size)\n",
    "    hidden_state, cell_state = initail_state\n",
    "    outputs = []\n",
    "\n",
    "    for X in inputs:\n",
    "        _, hidden_state, cell_state = lstm_cell(X, hidden_state, cell_state, parameters)\n",
    "    \n",
    "        outputs.append(output_cell(hidden_state, parameters))\n",
    "    return outputs, (hidden_state, cell_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个RNN 类来训练LSTM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModelScratch:\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.params = get_params(vocab_size, hidden_units, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "    \n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RNNModelScratch at 0x175f74c40>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModelScratch(vocab_size, hidden_units, device, initialize_parameters, init_lstm_state, lstm)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.char'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md2l\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m torch \u001b[38;5;28;01mas\u001b[39;00m d2l\n\u001b[1;32m      3\u001b[0m batch_size, num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m35\u001b[39m\n\u001b[1;32m      4\u001b[0m train_iter, vocab \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mload_data_time_machine(batch_size, num_steps)\n",
      "File \u001b[0;32m~/Desktop/Dev_env/miniconda3/envs/moshi/lib/python3.10/site-packages/d2l/torch.py:45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance_matrix\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n",
      "File \u001b[0;32m~/Desktop/Dev_env/miniconda3/envs/moshi/lib/python3.10/site-packages/scipy/__init__.py:78\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m _fun \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_fun, _types\u001b[38;5;241m.\u001b[39mModuleType):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Dev_env/miniconda3/envs/moshi/lib/python3.10/site-packages/numpy/__init__.py:370\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    368\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(w[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcategory\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mstr\u001b[39m(w[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage))\n\u001b[1;32m    369\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolyfit sanity test emitted a warning, most likely due \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto using a buggy Accelerate backend.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you compiled yourself, more information is available at:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mhttps://numpy.org/doc/stable/user/building.html#accelerated-blas-lapack-libraries\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    374\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOtherwise report this to the vendor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat provided NumPy.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(error_message))\n\u001b[1;32m    376\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _mac_os_check\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.char'"
     ]
    }
   ],
   "source": [
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "vocab_size, num_hiddens = len(vocab), 256\n",
    "num_epochs, lr = 500, 1\n",
    "\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corresonding embeddings for the batch dataset\n",
    "def get_embeddings(batch_dataset, embeddings):\n",
    "    embedding_dataset = np.matmul(batch_dataset, embeddings)\n",
    "    return embedding_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![forward propagation](../assets/forward-propagation.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2557073817.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[64], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# forward propagation\n",
    "def forward_propagation(batches, parameters, embeddings):\n",
    "    # get batch size\n",
    "    batch_size = batches[0].shape[0]\n",
    "\n",
    "    # 存储缓存信息\n",
    "    lstm_cache = dict()\n",
    "    hidden_cache = dict()\n",
    "    cell_cache = dict()\n",
    "    output_cache = dict()\n",
    "    embedding_cache = dict()\n",
    "\n",
    "    # 初始化hidden_state(h0), cell_state(c0)  偏置项\n",
    "    h0 = np.zeros([batch_size, hidden_units], dtype=np.float32)\n",
    "    c0 = np.zeros([batch_size, hidden_units], dtype=np.float32)\n",
    "\n",
    "    # 存储初始的hidden_state(h0), cell_state(c0)\n",
    "    hidden_cache['h0'] = h0\n",
    "    cell_cache['c0'] = c0\n",
    "\n",
    "    for i in range(len(batches) - 1):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
